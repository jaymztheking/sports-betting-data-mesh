[core]
# Set the executor to KubernetesExecutor
executor = KubernetesExecutor

# Enable support for Kubernetes namespace
kubernetes_namespace = airflow

# Use a PostgreSQL database for the backend
sql_alchemy_conn = postgresql+psycopg2://airflow:h4xorz@10.0.0.242:5432/airflowdb

# Configuration for logs
load_examples = True
base_url = http://10.0.0.243:8888
hostname_callable = socket.getfqdn

[webserver]
# Set the port for the Airflow web server
web_server_port = 8888

# Update the log URL template for better accessibility
log_url_template = http://10.0.0.243:8888/log/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}

# Authentication backend
rbac = True
authenticate = True

# Uncomment the line below if you're using a UI password authentication
# auth_backend = airflow.api.auth.backend.basic_auth

[kubernetes]
# Kubernetes-related configurations
namespace = airflow
worker_container_repository = apache/airflow
worker_container_tag = 2.7.2
delete_worker_pods = True
delete_worker_pods_on_failure = True

# Specify the volume to be mounted to workers
worker_pod_template = /opt/airflow/pod_template.yaml
volume_mounts = airflow-dags

# Connection settings for Kubernetes API
in_cluster = True
cluster_context = airflow

[logging]
# Configuration for persistent logging
remote_logging = True
remote_log_conn_id = minio
remote_base_log_folder = s3://airflow-logs/
encrypt_s3_logs = False

# Local log storage
base_log_folder = /opt/airflow/logs

[scheduler]
# Max threads for the scheduler
max_threads = 2

# DAG directory
dags_folder = /opt/airflow/dags

[metrics]
# Enable Prometheus metrics for monitoring
statsd_on = False